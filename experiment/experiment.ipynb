{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e93133c-710b-40f6-99c7-4bab03441763",
   "metadata": {},
   "source": [
    "### Template matching (POC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43537460-dce5-4c11-9f27-86394e6fcec9",
   "metadata": {},
   "source": [
    "#### template matching is a technique of finding matching feature in 2 images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdebdfd9-a63f-46cb-8329-93008bb37c19",
   "metadata": {},
   "source": [
    "![alt text](examples/example.webp \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa82ec3-3d5b-412e-baa8-4f22d3c29245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b350f14-e292-4592-8bd7-ee120d65a200",
   "metadata": {},
   "source": [
    "##### In this usecase we can use CNN based model to capture and extract features in an image like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5831dc-576e-4390-8cb1-0d55f8d1bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.Sequential([\n",
    "#         tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling2D(),\n",
    "#         tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "#         ..., \n",
    "#         ...,\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf4b96-6848-4dac-9afc-54a20e8d1ef1",
   "metadata": {},
   "source": [
    "##### Alternate and better approch to this problem is rather than training a model from scratch trained on limited data we can use other pretrained CNN based model like Resnet-50 or VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062873-be3d-4a5c-a734-739a24d70a2a",
   "metadata": {},
   "source": [
    "### Final Desicion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719d377-55ac-4108-a850-94e8424d655e",
   "metadata": {},
   "source": [
    "##### Using Resnet or VGG models works but only in static window means, scale of query image and backgroud image cannot be changed else the image wont get detected and since image data availability of a single product is minimum and 'one-shot learning' technique is suited, so Yolo model is selected with one-shot simulated approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a2f5f-5cf9-474e-9604-840e7e03a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def generate_multiclass_synthetic(\n",
    "    query_root, bg_dir, output_dir, num_images=500):\n",
    "    \n",
    "    os.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels/train\", exist_ok=True)\n",
    "\n",
    "    \"\"\"\n",
    "    1. take a main directory inside it sub directories each with each products image examples \n",
    "    2. create dictionary with sub directory and path of all images inside it eg: {\"class_name\":[path1,path,...]}\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    class_folders = sorted([d for d in os.listdir(query_root) if os.path.isdir(os.path.join(query_root, d))])\n",
    "    class_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "    print(f\"Detected {len(class_map)} classes: {class_map}\")\n",
    "\n",
    "    \"\"\"\n",
    "    3. for n times choose random background images and find its shape like (24,24,3) use it to find height and width\n",
    "    4. take random k (1,5) for each image in k is random sub dir name and random image in it \n",
    "    5. take height and width of background and find random scale (0.4,0.6) to scale product random image height and width use it to resize product image \n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # List background images\n",
    "        bg_files = [f for f in os.listdir(bg_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        bg_path = random.choice(bg_files)\n",
    "        bg_img = cv2.imread(os.path.join(bg_dir, bg_path))\n",
    "\n",
    "        if bg_img is None:\n",
    "            print(f\"error could not load background {bg_path}\")\n",
    "            continue\n",
    "\n",
    "        h_bg, w_bg = bg_img.shape[:2]\n",
    "        annotations = []\n",
    " \n",
    "        num_objects = random.randint(1, 5)\n",
    "        for _ in range(num_objects):\n",
    "            class_name = random.choice(class_folders)\n",
    "            class_id = class_map[class_name]\n",
    "            class_dir = os.path.join(query_root, class_name)\n",
    "            samples = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if not samples:\n",
    "                continue\n",
    "\n",
    "            sample_img = random.choice(samples)\n",
    "            query_path = os.path.join(class_dir, sample_img)\n",
    "            query_img = cv2.imread(query_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "            if query_img is None:\n",
    "                print(f\"error could not load {query_path}\")\n",
    "                continue\n",
    "\n",
    "            h_q, w_q = query_img.shape[:2]\n",
    "            scale = random.uniform(0.4, 0.6)\n",
    "            new_w = int(w_q * scale)\n",
    "            new_h = int(h_q * scale)\n",
    "\n",
    "            if new_w >= w_bg or new_h >= h_bg:\n",
    "                continue\n",
    "\n",
    "            query_resized = cv2.resize(query_img, (new_w, new_h))\n",
    "\n",
    "            \"\"\" \n",
    "            \n",
    "            6. use resize product image to replace the original image portion by taking a random point such that image wont go outside the backgroud \n",
    "            7. also save the annotation x-centered, y-centered, width, height relative to backgroud image\n",
    "\n",
    "            \"\"\"\n",
    "            x = random.randint(0, w_bg - new_w)\n",
    "            y = random.randint(0, h_bg - new_h)\n",
    "\n",
    "            \n",
    "            bg_img[y:y+new_h, x:x+new_w] = query_resized\n",
    "            x_center = (x + new_w / 2) / w_bg\n",
    "            y_center = (y + new_h / 2) / h_bg\n",
    "            width = new_w / w_bg\n",
    "            height = new_h / h_bg\n",
    "            annotations.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "        \"\"\" \n",
    "        8. use the annotation and save the model and annotation under same number different directory \"images/train\" and \"labels/train\" to make it yolo friendly\n",
    "        \"\"\"\n",
    "        img_name = f\"img_{i:05}.jpg\"\n",
    "        label_name = f\"img_{i:05}.txt\"\n",
    "        img_path = f\"{output_dir}/images/train/{img_name}\"\n",
    "        label_path = f\"{output_dir}/labels/train/{label_name}\"\n",
    "\n",
    "        cv2.imwrite(img_path, bg_img)\n",
    "\n",
    "        with open(label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(annotations) + \"\\n\")\n",
    "\n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121e58ac-5cb6-44f2-92f6-8f308c952326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 classes: {'cheerios': 0, 'mini_wheats': 1}\n"
     ]
    }
   ],
   "source": [
    "class_map = generate_multiclass_synthetic(\n",
    "    query_root=\"examples/classes/\",\n",
    "    bg_dir=\"examples/images/\",\n",
    "    output_dir=\"examples/yolo_data/\",\n",
    "    num_images=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d1c5d6-d233-499e-b8a2-0af88f5cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def write_data_yaml(class_map, output_path=\"examples/yolo_data/data.yaml\"):\n",
    "    data = {\n",
    "       \n",
    "        \"train\": \"images/train\",\n",
    "        \"val\": \"images/train\",\n",
    "        \"nc\": len(class_map),\n",
    "        \"names\": list(class_map.keys())\n",
    "    }\n",
    "    with open(output_path, \"w\") as f:\n",
    "        yaml.dump(data, f)\n",
    "\n",
    "write_data_yaml(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077c666-80c4-4357-9949-cce7b0c7a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1263a47-1df2-4861-9aa5-23c7910c548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo detect train model=yolov8n.pt data=examples/yolo_data/data.yaml epochs=5 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978298a-7917-40c6-9d70-d97965aca305",
   "metadata": {},
   "source": [
    "#### Yolo documentation : https://docs.ultralytics.com/tasks/detect/#predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7559549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.175 üöÄ Python-3.12.3 torch-2.5.1+cu124 CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 580.7¬±19.7 MB/s, size: 3747.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/rijo/Documents/Cprism/experiment/examples/yolo_data/labels/train.cache... 500 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<?, ?it/s]\n",
      "/home/rijo/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [01:05<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        500       1389      0.999      0.978      0.991      0.972\n",
      "              cheerios        376        661      0.999      0.991      0.995      0.986\n",
      "           mini_wheats        396        728      0.999      0.964      0.987      0.959\n",
      "Speed: 1.1ms preprocess, 48.2ms inference, 0.0ms loss, 4.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.98565,      0.9587])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71386acf",
   "metadata": {},
   "source": [
    "#### This model is trained on 5 epochs of 500 synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a7dca3f-a2a5-49d9-9555-03003280289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/rijo/Documents/Cprism/experiment/examples/shelves.jpg: 736x1280 4 cheerioss, 13 mini_wheatss, 219.9ms\n",
      "Speed: 31.1ms preprocess, 219.9ms inference, 9.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "results = model(\"examples/shelves.jpg\", \n",
    "                imgsz=1280,       \n",
    "                iou=0.5,          \n",
    "                conf=0.3,        \n",
    "                agnostic_nms=True, \n",
    "                max_det=100)      \n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box output\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "829f8462-44bf-4bbd-a72a-606895025c06",
   "metadata": {},
   "source": [
    "<img src=\"tmp9y3n4mx4.png\" alt=\"alt text\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf943b10-9185-4dfe-9fca-4c59bed0d235",
   "metadata": {},
   "source": [
    "### Log model to Dagshub (Mlflow in an instance for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef768059",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.box.map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77cbc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_params = {\n",
    "    \"imgsz\": 1280,\n",
    "    \"iou\": 0.5,\n",
    "    \"conf\": 0.3,\n",
    "    \"agnostic_nms\": True,\n",
    "    \"max_det\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c46f4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"slalrijo2005/Cprism\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"slalrijo2005/Cprism\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository slalrijo2005/Cprism initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository slalrijo2005/Cprism initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run first_iter at: https://dagshub.com/slalrijo2005/Cprism.mlflow/#/experiments/0/runs/698e7a54ae104b9bad9d5ff90c126b35\n",
      "üß™ View experiment at: https://dagshub.com/slalrijo2005/Cprism.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "\n",
    "dagshub.init(repo_owner='slalrijo2005', repo_name='Cprism', mlflow=True)\n",
    "URI = \"https://dagshub.com/slalrijo2005/Cprism.mlflow\"\n",
    "\n",
    "mlflow.set_tracking_uri(URI)\n",
    "mlflow.set_experiment(\"Product_detection\")\n",
    "\n",
    "model_path = \"runs/detect/train/weights/best.pt\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"first_iter\"):\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model_one\")  # logs the .pt file\n",
    "    mlflow.log_param(\"model_type\", \"YOLOv8n\")\n",
    "    mlflow.log_param(\"format\", \".pt\")\n",
    "    mlflow.log_params(inference_params)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca09a64-a6ef-4ca2-a6d1-038288e1f733",
   "metadata": {},
   "source": [
    "#### Register model (Not useful in this case you can only load from experiment even after register because model is in form of artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf409010-73f8-4fc4-87a1-1267244931c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Product_detection'.\n",
      "2025/08/07 19:00:22 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Product_detection, version 1\n",
      "Created version '1' of model 'Product_detection'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1754573422529, current_stage='None', description='', last_updated_timestamp=1754573422529, name='Product_detection', run_id='23f72a8af2fc4f68a1a5fc28d779ca20', run_link='', source='mlflow-artifacts:/7b9e474cfffb45179c7d571c32ad3b15/23f72a8af2fc4f68a1a5fc28d779ca20/artifacts/model_one', status='READY', status_message=None, tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_run_id = \"23f72a8af2fc4f68a1a5fc28d779ca20\" #secret (revealed temp for machine task)\n",
    "model_artifact_path = \"model_one\"\n",
    "model_uri = f\"runs:/{model_run_id}/{model_artifact_path}\"\n",
    "mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=\"Product_detection\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28511d84-ef9e-4f7b-99b5-409fb8191812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716000940ab24c3483f61a5df38ca49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artifact_uri = \"mlflow-artifacts:/7b9e474cfffb45179c7d571c32ad3b15/23f72a8af2fc4f68a1a5fc28d779ca20/artifacts/model_one/best.pt\" #secret (revealed temp for machine task)\n",
    "loaded_model = mlflow.artifacts.download_artifacts(artifact_uri=artifact_uri)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b263f64-4551-4f41-b782-6b979092a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = YOLO(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60a66bed-69a4-4712-acf7-ffc638ef06ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rijo/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/rijo/Documents/Cprism/experiment/examples/shelves.jpg: 736x1280 4 cheerioss, 13 mini_wheatss, 327.6ms\n",
      "Speed: 38.5ms preprocess, 327.6ms inference, 7.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "results = loaded_model(\"examples/shelves.jpg\", \n",
    "                imgsz=1280,       \n",
    "                iou=0.5,          \n",
    "                conf=0.3,        \n",
    "                agnostic_nms=True, \n",
    "                max_det=100)      \n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box output\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea4b11-044f-47d8-a276-e7db3f79af84",
   "metadata": {},
   "source": [
    "### Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05296704-2659-40de-8f41-dffd77d391cd",
   "metadata": {},
   "source": [
    "#### Better approch that is more stable and zero shot will be image feature embedding model with sliding window approch on different scale but need more that 50hrs to experiment and build a production level model eg: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc00c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca787c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
